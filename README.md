# üìö Deep Learning Reading List (Inspired by Ilya Sutskever)
>
> Ilya Sutskever once told John Carmack:  
> ‚ÄúIf you really learn all of these, you‚Äôll know 90% of what matters today.‚Äù  

This repository organizes that legendary reading list ‚Äî plus context, summaries, and extra links.  

The goal: not just a flat list, but a structured learning path for learning Artificial Intelligence, Machine Learning and Deep Learning.

---

## üîó References
- [Original Twitter Mention](https://twitter.com/keshavchan/status/1787861946173186062)  
- [Arc.net Compilation](https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE)  

---

## üìñ Study Roadmap
Suggested order to maximize learning:

1. **Foundations & Theory** ‚Üí compression, MDL, complexity  
2. **CNNs & Vision** ‚Üí AlexNet, ResNets, dilated convs  
3. **RNNs & Sequence Models** ‚Üí LSTMs, RNN regularization, seq2seq  
4. **Attention & Transformers** ‚Üí Bahdanau, Transformer, Annotated Transformer  
5. **Scaling & Systems** ‚Üí GPipe, scaling laws, large-scale speech models  
6. **Relational & Reasoning** ‚Üí Neural Turing Machines, relational modules  
7. **Bonus Theory** ‚Üí Kolmogorov complexity, machine superintelligence  

---

## üóÇÔ∏è Reading List (with Context)

### 1. Foundations & Theory
- **Keeping Neural Networks Simple** ‚Äî Hinton & van Camp  
  *Model compression, minimum description length.* [[pdf]](https://www.cs.toronto.edu/~hinton/absps/colt93.pdf)  
- **Tutorial on MDL Principle** ‚Äî Grunwald  
  *Statistical model selection via compression.* [[pdf]](https://arxiv.org/pdf/math/0406077)  
- **Kolmogorov Complexity & Algorithmic Randomness** ‚Äî Shen, Uspensky, Vereshchagin  
  *Theoretical backbone of compression and randomness.* [[pdf]](https://www.lirmm.fr/~ashen/kolmbook-eng-scan.pdf)  

### 2. CNNs & Vision
- **ImageNet Classification with Deep CNNs (AlexNet)** ‚Äî Krizhevsky et al. [[pdf]](https://sing.stanford.edu/curis-fellowships/rh/vision-dnn.pdf)  
- **Deep Residual Learning (ResNet)** ‚Äî He et al. [[arXiv]](https://arxiv.org/abs/1512.03385)  
- **Identity Mappings in ResNets** ‚Äî He et al. [[arXiv]](https://arxiv.org/abs/1603.05027)  
- **Dilated Convolutions** ‚Äî Yu & Koltun [[arXiv]](https://arxiv.org/abs/1511.07122)  

### 3. RNNs & Sequence Models
- **Unreasonable Effectiveness of RNNs** ‚Äî Karpathy [[Blog]](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)  
- **Understanding LSTM Networks** ‚Äî Olah [[Blog]](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)  
- **RNN Regularization** ‚Äî Zaremba et al. [[arXiv]](https://arxiv.org/abs/1409.2329)  
- **Seq2Seq for Sets** ‚Äî Vinyals et al. [[arXiv]](https://arxiv.org/abs/1511.06391)  
- **Pointer Networks** ‚Äî Vinyals et al. [[arXiv]](https://arxiv.org/abs/1506.03134)  
- **Neural Machine Translation (Attention)** ‚Äî Bahdanau et al. [[arXiv]](https://arxiv.org/abs/1409.0473)  

### 4. Attention & Transformers
- **Attention Is All You Need (Transformer)** ‚Äî Vaswani et al. [[arXiv]](https://arxiv.org/abs/1706.03762)  
- **The Annotated Transformer** ‚Äî Rush et al. [[Blog]](https://nlp.seas.harvard.edu/annotated-transformer/) [[GitHub]](https://github.com/harvardnlp/annotated-transformer/)  

### 5. Scaling & Systems
- **GPipe: Pipeline Parallelism** ‚Äî Huang et al. [[arXiv]](https://arxiv.org/abs/1811.06965v5)  
- **Deep Speech 2** ‚Äî Amodei et al. [[arXiv]](https://arxiv.org/abs/1512.02595)  
- **Scaling Laws for Language Models** ‚Äî Kaplan et al. [[arXiv]](https://arxiv.org/abs/2001.08361)  

### 6. Relational & Reasoning
- **Neural Turing Machines** ‚Äî Graves et al. [[arXiv]](https://arxiv.org/abs/1410.5401)  
- **Relational Reasoning Module** ‚Äî Santoro et al. [[arXiv]](https://arxiv.org/abs/1706.01427)  
- **Relational RNNs** ‚Äî Santoro et al. [[arXiv]](https://arxiv.org/abs/1806.01822)  
- **Variational Lossy Autoencoder** ‚Äî Xi Chen et al. [[arXiv]](https://arxiv.org/abs/1611.02731)  
- **Message Passing Neural Nets for Quantum Chemistry** ‚Äî Gilmer et al. [[arXiv]](https://arxiv.org/abs/1704.01212)  

### 7. Bonus Theory & Philosophy
- **First Law of Complexodynamics** ‚Äî Scott Aaronson [[Blog]](https://scottaaronson.blog/?p=762)  
- **Coffee Automaton (Complexity in Closed Systems)** ‚Äî Aaronson et al. [[arXiv]](https://arxiv.org/abs/1405.6903)  
- **Machine Super Intelligence** ‚Äî Shane Legg [[pdf]](https://pdfs.semanticscholar.org/e758/b579456545f8691bbadaf26bcd3b536c7172.pdf)  
- **CS231n Course Notes** ‚Äî Stanford [[Notes]](https://cs231n.github.io/)  

---

## üî• Extras
To modernize the list, you may want to add:
- **Diffusion Models (Ho et al., 2020)** [[arXiv]](https://arxiv.org/abs/2006.11239)  
- **Vision Transformers (Dosovitskiy et al., 2020)** [[arXiv]](https://arxiv.org/abs/2010.11929)  
- **Chain-of-Thought Prompting (Wei et al., 2022)** [[arXiv]](https://arxiv.org/abs/2201.11903)  

---

## üè∑Ô∏è Tags
`deep-learning` `transformers` `cnn` `rnn` `scaling-laws` `theory` `ai-history`  
